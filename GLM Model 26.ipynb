{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StateFarm MLE Take Home\n",
    "\n",
    "Start Time: 10:30am\n",
    "\n",
    "End Time:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeremydemlow/miniconda3/envs/statefarm/lib/python3.7/site-packages/statsmodels/compat/pandas.py:49: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  data_klasses = (pandas.Series, pandas.DataFrame, pandas.Panel)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-23 13:47:28 Training set: (32000, 100), (32000, 1)\n",
      "2023-11-23 13:47:28 Validation set: (4000, 100), (4000, 1)\n",
      "2023-11-23 13:47:28 Test set: (4000, 100), (4000, 1)\n",
      "2023-11-23 13:47:29 Data Set Sizes After Preprocessing..\n",
      "2023-11-23 13:47:29 Training set: (32000, 122)\n",
      "2023-11-23 13:47:29 Validation set: (4000, 122)\n",
      "2023-11-23 13:47:29 Test set: (4000, 122)\n",
      "2023-11-23 13:47:29 Selected Variables: ['x5_saturday', 'x81_July', 'x81_December', 'x31_japan', 'x81_October', 'x5_sunday', 'x31_asia', 'x81_February', 'x91', 'x81_May', 'x5_monday', 'x81_September', 'x81_March', 'x53', 'x81_November', 'x44', 'x81_June', 'x12', 'x5_tuesday', 'x81_August', 'x81_January', 'x62', 'x31_germany', 'x58', 'x56']\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.536475\n",
      "         Iterations 6\n",
      "2023-11-23 13:47:30                            Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   No. Observations:                40000\n",
      "Model:                          Logit   Df Residuals:                    39975\n",
      "Method:                           MLE   Df Model:                           24\n",
      "Date:                Fri, 17 Nov 2023   Pseudo R-squ.:                  0.2260\n",
      "Time:                        13:47:30   Log-Likelihood:                -21459.\n",
      "converged:                       True   LL-Null:                       -27725.\n",
      "                                        LLR p-value:                     0.000\n",
      "=================================================================================\n",
      "                    coef    std err          z      P>|z|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------\n",
      "x5_saturday      -1.2065      0.037    -32.949      0.000      -1.278      -1.135\n",
      "x81_July          1.0958      0.047     23.090      0.000       1.003       1.189\n",
      "x81_December      1.0294      0.047     21.812      0.000       0.937       1.122\n",
      "x31_japan         0.9249      0.056     16.540      0.000       0.815       1.035\n",
      "x81_October       0.9111      0.047     19.516      0.000       0.820       1.003\n",
      "x5_sunday        -0.8573      0.036    -23.921      0.000      -0.928      -0.787\n",
      "x31_asia         -0.7595      0.030    -25.522      0.000      -0.818      -0.701\n",
      "x81_February      0.8253      0.047     17.645      0.000       0.734       0.917\n",
      "x91               0.7660      0.013     60.549      0.000       0.741       0.791\n",
      "x81_May           0.7930      0.047     16.837      0.000       0.701       0.885\n",
      "x5_monday        -0.6696      0.035    -18.935      0.000      -0.739      -0.600\n",
      "x81_September     0.6814      0.047     14.630      0.000       0.590       0.773\n",
      "x81_March         0.6835      0.047     14.664      0.000       0.592       0.775\n",
      "x53              -0.6221      0.012    -50.365      0.000      -0.646      -0.598\n",
      "x81_November      0.5608      0.047     12.039      0.000       0.470       0.652\n",
      "x44              -0.5058      0.012    -41.482      0.000      -0.530      -0.482\n",
      "x81_June          0.4448      0.047      9.537      0.000       0.353       0.536\n",
      "x12              -0.3938      0.012    -32.788      0.000      -0.417      -0.370\n",
      "x5_tuesday       -0.3727      0.035    -10.505      0.000      -0.442      -0.303\n",
      "x81_August        0.4300      0.047      9.162      0.000       0.338       0.522\n",
      "x81_January       0.3457      0.047      7.373      0.000       0.254       0.438\n",
      "x62              -0.2882      0.012    -23.503      0.000      -0.312      -0.264\n",
      "x31_germany      -0.1664      0.029     -5.649      0.000      -0.224      -0.109\n",
      "x58               0.2107      0.012     17.516      0.000       0.187       0.234\n",
      "x56               0.2002      0.012     16.356      0.000       0.176       0.224\n",
      "=================================================================================\n",
      "2023-11-23 13:47:30 The C-Statistics is 0.8011297055284153\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import statsmodels.api as sm\n",
    "import bokeh\n",
    "import collections as ccc\n",
    "import logging\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import logging\n",
    "\n",
    "date_strftime_format = \"%Y-%m-%y %H:%M:%S\"\n",
    "logging.basicConfig(\n",
    "    stream=sys.stdout,\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s %(message)s\",\n",
    "    datefmt=date_strftime_format,\n",
    ")\n",
    "\n",
    "\n",
    "class DataSplitter:\n",
    "    \"\"\"\n",
    "    A class for splitting data into training, validation, and optionally test sets.\n",
    "\n",
    "    Attributes:\n",
    "        df (pandas.DataFrame): The complete dataset.\n",
    "        y_vars (list of str): Column names of the target variables.\n",
    "        splits (list of lists): Indices for training, validation, and test sets.\n",
    "        X_train, X_valid, X_test (pandas.DataFrame): Training, validation, and test features.\n",
    "        y_train, y_valid, y_test (pandas.DataFrame): Training, validation, and test targets.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df, y_vars):\n",
    "        self.df = df\n",
    "        self.y_vars = y_vars\n",
    "        self.splits = {\"train\": None, \"valid\": None, \"test\": None}\n",
    "        self.X_train = self.X_valid = self.X_test = None\n",
    "        self.y_train = self.y_valid = self.y_test = None\n",
    "\n",
    "    def split_data(\n",
    "        self, test_size=0.2, val_size=0.1, random_state=13, create_test_set=False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Splits the dataset into training, validation, and optionally test sets.\n",
    "\n",
    "        The method first splits the dataset into training plus temporary set and validation set.\n",
    "        If a test set is requested, it further splits the training plus temporary set into\n",
    "        the final training set and test set. The 'test_size' parameter can be either a float\n",
    "        to represent the proportion of the dataset to include in the test split or an\n",
    "        absolute number of samples.\n",
    "\n",
    "        Parameters:\n",
    "            test_size (float or int): If float, represents the proportion of the dataset\n",
    "                                      to include in the test split. If int, represents\n",
    "                                      the absolute number of samples to include in the test split.\n",
    "            val_size (float): Proportion of the dataset to include in the validation split.\n",
    "            random_state (int): Controls the shuffling applied to the data before applying the split.\n",
    "            create_test_set (bool): Whether to create a separate test set. If False,\n",
    "                                    the test set-related attributes (X_test, y_test) remain None.\n",
    "\n",
    "        Note:\n",
    "            - The 'test_size' is interpreted as a proportion if it is a float less than 1,\n",
    "              otherwise, it is interpreted as the absolute number of samples.\n",
    "            - The actual size of the test set might be slightly different from the specified 'test_size'\n",
    "              when it is given as a proportion, due to rounding.\n",
    "        \"\"\"\n",
    "        x_train_full, x_val, y_train_full, y_val = train_test_split(\n",
    "            self.df.drop(columns=self.y_vars),\n",
    "            self.df[self.y_vars],\n",
    "            test_size=val_size,\n",
    "            random_state=random_state,\n",
    "        )\n",
    "\n",
    "        if create_test_set:\n",
    "            x_train, x_test, y_train, y_test = train_test_split(\n",
    "                x_train_full,\n",
    "                y_train_full,\n",
    "                test_size=test_size,\n",
    "                random_state=random_state,\n",
    "            )\n",
    "            self.splits[\"test\"] = (x_test.index, y_test.index)\n",
    "        else:\n",
    "            x_train, y_train = x_train_full, y_train_full\n",
    "\n",
    "        self.splits[\"train\"] = (x_train.index, y_train.index)\n",
    "        self.splits[\"valid\"] = (x_val.index, y_val.index)\n",
    "\n",
    "        self._extract_splits()\n",
    "\n",
    "    def _extract_splits(self):\n",
    "        \"\"\"Internal method to extract features and targets for each set based on splits.\"\"\"\n",
    "        self.X_train, self.y_train = (\n",
    "            self.df.drop(columns=self.y_vars).iloc[self.splits[\"train\"][0]],\n",
    "            self.df[self.y_vars].iloc[self.splits[\"train\"][1]],\n",
    "        )\n",
    "        self.X_valid, self.y_valid = (\n",
    "            self.df.drop(columns=self.y_vars).iloc[self.splits[\"valid\"][0]],\n",
    "            self.df[self.y_vars].iloc[self.splits[\"valid\"][1]],\n",
    "        )\n",
    "\n",
    "        logging.info(f\"Training set: {self.X_train.shape}, {self.y_train.shape}\")\n",
    "        logging.info(f\"Validation set: {self.X_valid.shape}, {self.y_valid.shape}\")\n",
    "\n",
    "        if self.splits[\"test\"]:\n",
    "            self.X_test, self.y_test = (\n",
    "                self.df.drop(columns=self.y_vars).iloc[self.splits[\"test\"][0]],\n",
    "                self.df[self.y_vars].iloc[self.splits[\"test\"][1]],\n",
    "            )\n",
    "            logging.info(f\"Test set: {self.X_test.shape}, {self.y_test.shape}\")\n",
    "        else:\n",
    "            logging.info(\"No test set created.\")\n",
    "\n",
    "\n",
    "class DataPreprocessor:\n",
    "    \"\"\"\n",
    "    A class for preprocessing data for machine learning tasks.\n",
    "\n",
    "    This class handles the conversion of monetary and percentage string values to floats,\n",
    "    imputation of missing values, scaling of features, and the creation of dummy variables\n",
    "    for categorical columns. It can be used for both fitting and transforming training data,\n",
    "    as well as transforming new data with the same transformations applied to the training data.\n",
    "\n",
    "    Attributes:\n",
    "        columns_to_convert (list of str): Columns that contain monetary or percentage string values.\n",
    "        columns_to_impute (list of str): Columns for which missing values will be imputed.\n",
    "        columns_to_dummy (list of str): Categorical columns to be converted to dummy variables.\n",
    "        target_column (str, optional): The name of the target variable column. Default is None.\n",
    "        imputer (SimpleImputer): The imputer object used for missing value imputation.\n",
    "        scaler (StandardScaler): The scaler object used for feature scaling.\n",
    "        dummy_columns (dict): A dictionary to store the columns created after dummy encoding.\n",
    "\n",
    "    Methods:\n",
    "        fit_transform(df): Fits the preprocessor to the data and transforms the data.\n",
    "        transform(df): Transforms a new dataset using the transformations fitted on the training data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        columns_to_convert,\n",
    "        columns_to_impute,\n",
    "        columns_to_dummy,\n",
    "        target_column=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the DataPreprocessor with specified columns for conversion, imputation,\n",
    "        dummy variable creation, and optionally a target column.\n",
    "\n",
    "        Parameters:\n",
    "            columns_to_convert (list of str): Columns with monetary or percentage string values to convert.\n",
    "            columns_to_impute (list of str): Columns for which missing values will be imputed.\n",
    "            columns_to_dummy (list of str): Categorical columns to be converted into dummy variables.\n",
    "            target_column (str, optional): The name of the target variable column. Default is None.\n",
    "        \"\"\"\n",
    "        self.columns_to_convert = columns_to_convert\n",
    "        self.columns_to_impute = columns_to_impute\n",
    "        self.columns_to_dummy = columns_to_dummy\n",
    "        self.target_column = target_column\n",
    "        self.imputer = SimpleImputer(missing_values=np.nan, strategy=\"mean\")\n",
    "        self.scaler = StandardScaler()\n",
    "        self.dummy_columns = {}\n",
    "\n",
    "    def _convert_columns(self, df):\n",
    "        \"\"\"\n",
    "        Converts monetary and percentage values in specified columns from string to float.\n",
    "\n",
    "        Parameters:\n",
    "            df (pandas.DataFrame): The dataframe to process.\n",
    "\n",
    "        Returns:\n",
    "            pandas.DataFrame: The dataframe with converted columns.\n",
    "        \"\"\"\n",
    "\n",
    "        for col in self.columns_to_convert:\n",
    "            if df[col].dtype == object:\n",
    "                df[col] = (\n",
    "                    df[col]\n",
    "                    .replace(\n",
    "                        {r\"\\$\": \"\", \",\": \"\", \"%\": \"\", r\"\\(\": \"-\", r\"\\)\": \"\"}, regex=True\n",
    "                    )\n",
    "                    .astype(float)\n",
    "                )\n",
    "            else:\n",
    "                logging.warning(\n",
    "                    f\"Column {col} is not of string type and will not be converted.\"\n",
    "                )\n",
    "        return df\n",
    "\n",
    "    def fit_transform(self, df):\n",
    "        \"\"\"\n",
    "        Fits the preprocessor to the data and transforms the data.\n",
    "        This includes converting specified columns, imputing missing values,\n",
    "        scaling, and creating dummy variables.\n",
    "\n",
    "        Parameters:\n",
    "            df (pandas.DataFrame): The training dataset to fit and transform.\n",
    "\n",
    "        Returns:\n",
    "            pandas.DataFrame: The transformed dataframe.\n",
    "        \"\"\"\n",
    "        df = self._convert_columns(df)\n",
    "        columns_to_drop = self.columns_to_dummy[:]\n",
    "        if self.target_column and self.target_column in df.columns:\n",
    "            columns_to_drop.append(self.target_column)\n",
    "\n",
    "        df_imputed = pd.DataFrame(\n",
    "            self.imputer.fit_transform(df.drop(columns=columns_to_drop)),\n",
    "            columns=df.drop(columns=columns_to_drop).columns,\n",
    "            index=df.index,\n",
    "        )\n",
    "        df_imputed_std = pd.DataFrame(\n",
    "            self.scaler.fit_transform(df_imputed),\n",
    "            columns=df_imputed.columns,\n",
    "            index=df.index,\n",
    "        )\n",
    "\n",
    "        for col in self.columns_to_dummy:\n",
    "            dummies = pd.get_dummies(\n",
    "                df[col], drop_first=True, prefix=col, prefix_sep=\"_\", dummy_na=True\n",
    "            )\n",
    "            dummies = dummies.reindex(df.index, fill_value=0)\n",
    "            self.dummy_columns[col] = dummies.columns.tolist()\n",
    "            df_imputed_std = pd.concat([df_imputed_std, dummies], axis=1, sort=False)\n",
    "\n",
    "        return df_imputed_std\n",
    "\n",
    "    def transform(self, df):\n",
    "        \"\"\"\n",
    "        Transforms a dataset using the transformations fitted on the training data.\n",
    "        This includes converting specified columns, imputing missing values,\n",
    "        scaling, and creating dummy variables based on the training data.\n",
    "\n",
    "        Parameters:\n",
    "            df (pandas.DataFrame): The new dataset to transform.\n",
    "\n",
    "        Returns:\n",
    "            pandas.DataFrame: The transformed dataframe.\n",
    "        \"\"\"\n",
    "        df = self._convert_columns(df)\n",
    "        columns_to_drop = self.columns_to_dummy[:]\n",
    "        if self.target_column and self.target_column in df.columns:\n",
    "            columns_to_drop.append(self.target_column)\n",
    "        df_imputed = pd.DataFrame(\n",
    "            self.imputer.transform(df.drop(columns=columns_to_drop)),\n",
    "            columns=df.drop(columns=columns_to_drop).columns,\n",
    "            index=df.index,\n",
    "        )\n",
    "        df_imputed_std = pd.DataFrame(\n",
    "            self.scaler.transform(df_imputed),\n",
    "            columns=df_imputed.columns,\n",
    "            index=df.index,\n",
    "        )\n",
    "\n",
    "        for col in self.columns_to_dummy:\n",
    "            dummies = pd.get_dummies(\n",
    "                df[col], drop_first=True, prefix=col, prefix_sep=\"_\", dummy_na=True\n",
    "            )\n",
    "            dummies = dummies.reindex(columns=self.dummy_columns[col], fill_value=0)\n",
    "            df_imputed_std = pd.concat([df_imputed_std, dummies], axis=1, sort=False)\n",
    "\n",
    "        return df_imputed_std\n",
    "\n",
    "\n",
    "class LogisticRegressionAnalysis:\n",
    "    \"\"\"\n",
    "    A class for conducting logistic regression analysis.\n",
    "\n",
    "    This class simplifies the process of fitting a logistic regression model,\n",
    "    selecting important variables based on coefficients, and evaluating the model's\n",
    "    performance using the C-statistic (ROC AUC score).\n",
    "\n",
    "    Attributes:\n",
    "        exploratory_LR (LogisticRegression): The initial logistic regression model.\n",
    "        variables (list): List of selected variables based on the model's coefficients.\n",
    "        final_model (statsmodels.Logit): The final logistic regression model after variable selection.\n",
    "        final_result (statsmodels.LogitResults): Results of the final logistic regression model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initializes the LogisticRegressionAnalysis class with default values.\"\"\"\n",
    "        self.exploratory_LR = None\n",
    "        self.variables = []\n",
    "        self.final_model = None\n",
    "        self.final_result = None\n",
    "\n",
    "    def fit_exploratory_model(self, df, target_column):\n",
    "        \"\"\"\n",
    "        Fits an exploratory logistic regression model to identify important variables.\n",
    "\n",
    "        Args:\n",
    "            df (pandas.DataFrame): The dataset to fit the model on.\n",
    "            target_column (str): The name of the target variable in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            list: The list of top 25 variables selected based on the coefficients.\n",
    "        \"\"\"\n",
    "        self.exploratory_LR = LogisticRegression(\n",
    "            penalty=\"l1\", fit_intercept=False, solver=\"liblinear\"\n",
    "        )\n",
    "        self.exploratory_LR.fit(df.drop(columns=[target_column]), df[target_column])\n",
    "\n",
    "        # Extract coefficients\n",
    "        results = pd.DataFrame(df.drop(columns=[target_column]).columns).rename(\n",
    "            columns={0: \"name\"}\n",
    "        )\n",
    "        results[\"coefs\"] = self.exploratory_LR.coef_[0]\n",
    "        results[\"coefs_squared\"] = results[\"coefs\"] ** 2\n",
    "\n",
    "        # Select top 25 variables\n",
    "        self.variables = results.nlargest(25, \"coefs_squared\")[\"name\"].tolist()\n",
    "\n",
    "        logging.info(\"Selected Variables: %s\", self.variables)\n",
    "        return self.variables\n",
    "\n",
    "    def fit_final_model(self, df, target_column):\n",
    "        \"\"\"\n",
    "        Fits the final logistic regression model using selected variables.\n",
    "\n",
    "        Args:\n",
    "            df (pandas.DataFrame): The dataset to fit the model on.\n",
    "            target_column (str): The name of the target variable in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            str: The summary of the final logistic regression model.\n",
    "        \"\"\"\n",
    "        self.final_model = sm.Logit(df[target_column], df[self.variables])\n",
    "        self.final_result = self.final_model.fit()\n",
    "\n",
    "        logging.info(self.final_result.summary())\n",
    "        return self.final_result.summary()\n",
    "\n",
    "    def evaluate_model(self, df, target_column):\n",
    "        \"\"\"\n",
    "        Evaluates the model's performance using the C-statistic (ROC AUC score).\n",
    "\n",
    "        Args:\n",
    "            df (pandas.DataFrame): The dataset to evaluate the model on.\n",
    "            target_column (str): The name of the target variable in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            pandas.Series: Sum of target variable in each probability bin.\n",
    "        \"\"\"\n",
    "        outcomes = pd.DataFrame(self.final_result.predict(df[self.variables])).rename(\n",
    "            columns={0: \"probs\"}\n",
    "        )\n",
    "        outcomes[\"y\"] = df[target_column]\n",
    "\n",
    "        roc_auc = roc_auc_score(outcomes[\"y\"], outcomes[\"probs\"])\n",
    "        logging.info(\"The C-Statistics is %s\", roc_auc)\n",
    "\n",
    "        outcomes[\"prob_bin\"] = pd.qcut(outcomes[\"probs\"], q=20)\n",
    "        grouped_outcomes = outcomes.groupby([\"prob_bin\"])[\"y\"].sum()\n",
    "        return outcomes, grouped_outcomes\n",
    "\n",
    "\n",
    "train_val = pd.read_csv(\"statefarm/files/data/exercise_26_train.csv\")\n",
    "\n",
    "data_splitter = DataSplitter(df=train_val, y_vars=[\"y\"])\n",
    "data_splitter.split_data(\n",
    "    test_size=4000, val_size=0.1, random_state=13, create_test_set=True\n",
    ")\n",
    "\n",
    "X_train, y_train = data_splitter.X_train, data_splitter.y_train\n",
    "X_valid, y_valid = data_splitter.X_valid, data_splitter.y_valid\n",
    "X_test, y_test = data_splitter.X_test, data_splitter.y_test\n",
    "\n",
    "columns_to_convert = [\"x12\", \"x63\"]\n",
    "columns_to_impute = [\n",
    "    col\n",
    "    for col in train_val.columns\n",
    "    if col not in [\"y\", \"x5\", \"x31\", \"x81\", \"x82\"] + columns_to_convert\n",
    "]\n",
    "columns_to_dummy = [\"x5\", \"x31\", \"x81\", \"x82\"]\n",
    "\n",
    "preprocessor = DataPreprocessor(\n",
    "    columns_to_convert, columns_to_impute, columns_to_dummy, target_column=\"y\"\n",
    ")\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_valid = preprocessor.transform(X_valid)\n",
    "X_test = preprocessor.transform(X_test)\n",
    "\n",
    "train_df = pd.concat([X_train, y_train], axis=1, sort=False).reset_index(drop=True)\n",
    "valid_df = pd.concat([X_valid, y_valid], axis=1, sort=False).reset_index(drop=True)\n",
    "test_df = pd.concat([X_test, y_test], axis=1, sort=False).reset_index(drop=True)\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logging.info(\"Data Set Sizes After Preprocessing..\")\n",
    "logging.info(f\"Training set: {train_df.shape}\")\n",
    "logging.info(f\"Validation set: {valid_df.shape}\")\n",
    "logging.info(f\"Test set: {test_df.shape}\")\n",
    "\n",
    "lr_analysis = LogisticRegressionAnalysis()\n",
    "important_variables = lr_analysis.fit_exploratory_model(train_df, \"y\")\n",
    "combined_df = pd.concat([train_df, valid_df, test_df])\n",
    "final_model_summary = lr_analysis.fit_final_model(combined_df, \"y\")\n",
    "result, evaluation_result = lr_analysis.evaluate_model(combined_df, \"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x0',\n",
       " 'x1',\n",
       " 'x2',\n",
       " 'x3',\n",
       " 'x4',\n",
       " 'x6',\n",
       " 'x7',\n",
       " 'x8',\n",
       " 'x9',\n",
       " 'x10',\n",
       " 'x11',\n",
       " 'x13',\n",
       " 'x14',\n",
       " 'x15',\n",
       " 'x16',\n",
       " 'x17',\n",
       " 'x18',\n",
       " 'x19',\n",
       " 'x20',\n",
       " 'x21',\n",
       " 'x22',\n",
       " 'x23',\n",
       " 'x24',\n",
       " 'x25',\n",
       " 'x26',\n",
       " 'x27',\n",
       " 'x28',\n",
       " 'x29',\n",
       " 'x30',\n",
       " 'x32',\n",
       " 'x33',\n",
       " 'x34',\n",
       " 'x35',\n",
       " 'x36',\n",
       " 'x37',\n",
       " 'x38',\n",
       " 'x39',\n",
       " 'x40',\n",
       " 'x41',\n",
       " 'x42',\n",
       " 'x43',\n",
       " 'x44',\n",
       " 'x45',\n",
       " 'x46',\n",
       " 'x47',\n",
       " 'x48',\n",
       " 'x49',\n",
       " 'x50',\n",
       " 'x51',\n",
       " 'x52',\n",
       " 'x53',\n",
       " 'x54',\n",
       " 'x55',\n",
       " 'x56',\n",
       " 'x57',\n",
       " 'x58',\n",
       " 'x59',\n",
       " 'x60',\n",
       " 'x61',\n",
       " 'x62',\n",
       " 'x64',\n",
       " 'x65',\n",
       " 'x66',\n",
       " 'x67',\n",
       " 'x68',\n",
       " 'x69',\n",
       " 'x70',\n",
       " 'x71',\n",
       " 'x72',\n",
       " 'x73',\n",
       " 'x74',\n",
       " 'x75',\n",
       " 'x76',\n",
       " 'x77',\n",
       " 'x78',\n",
       " 'x79',\n",
       " 'x80',\n",
       " 'x83',\n",
       " 'x84',\n",
       " 'x85',\n",
       " 'x86',\n",
       " 'x87',\n",
       " 'x88',\n",
       " 'x89',\n",
       " 'x90',\n",
       " 'x91',\n",
       " 'x92',\n",
       " 'x93',\n",
       " 'x94',\n",
       " 'x95',\n",
       " 'x96',\n",
       " 'x97',\n",
       " 'x98',\n",
       " 'x99']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_to_impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "test = pd.read_csv(\"statefarm/files/data/exercise_26_test.csv\")\n",
    "MODEL_PATH = \"statefarm/files/models/logistic_regression_model.pkl\"\n",
    "PREPROCESSOR_PATH = \"statefarm/files/models/preprocessor.pkl\"\n",
    "\n",
    "model = joblib.load(MODEL_PATH)\n",
    "preprocessor = joblib.load(PREPROCESSOR_PATH)\n",
    "\n",
    "# Selecting the first two rows and dropping the target column 'y'\n",
    "sample_data = test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-23 14:27:40 Column x12 is not of string type and will not be converted.\n",
      "2023-11-23 14:27:40 Column x63 is not of string type and will not be converted.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'phat': 0.34711484677709, 'business_outcome': 0}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_df = pd.DataFrame([json.loads(sample_data.iloc[0].to_json())])\n",
    "\n",
    "response = pd.DataFrame(\n",
    "    model.final_result.predict(preprocessor.transform(sample_df)[model.variables])\n",
    ").rename(columns={0: \"phat\"})\n",
    "np.where(response[\"phat\"] >= 0.75, 1, 0)\n",
    "\n",
    "response[\"business_outcome\"] = np.where(response[\"phat\"] >= 0.75, 1, 0)\n",
    "\n",
    "response = response.to_dict(orient=\"records\")[0]\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_json_batch = sample_data.to_json(orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-23 14:19:00 Column x12 is not of string type and will not be converted.\n",
      "2023-11-23 14:19:00 Column x63 is not of string type and will not be converted.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\"phat\":{\"0\":0.3471148468},\"variable_name\":{\"0\":\"business_outcome\"}}'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['business_outcome'] = # where Phat >= 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['x0', 'x1', 'x2', 'x3', 'x4', 'x6', 'x7', 'x8', 'x9', 'x10', 'x11',\n",
      "       'x13', 'x14', 'x15', 'x16', 'x17', 'x18', 'x19', 'x20', 'x21', 'x22',\n",
      "       'x23', 'x24', 'x25', 'x26', 'x27', 'x28', 'x29', 'x30', 'x31', 'x32',\n",
      "       'x33', 'x34', 'x35', 'x36', 'x37', 'x38', 'x39', 'x40', 'x41', 'x42',\n",
      "       'x43', 'x44', 'x45', 'x46', 'x47', 'x48', 'x49', 'x50', 'x51', 'x52',\n",
      "       'x53', 'x54', 'x55', 'x56', 'x57', 'x58', 'x59', 'x60', 'x61', 'x62',\n",
      "       'x64', 'x65', 'x66', 'x67', 'x68', 'x69', 'x70', 'x71', 'x72', 'x73',\n",
      "       'x74', 'x75', 'x76', 'x77', 'x78', 'x79', 'x80', 'x83', 'x84', 'x85',\n",
      "       'x86', 'x87', 'x88', 'x89', 'x90', 'x91', 'x92', 'x93', 'x94', 'x95',\n",
      "       'x96', 'x97', 'x98', 'x99'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Find columns with null or missing values\n",
    "is_null_df = test.isnull()\n",
    "columns_with_null = is_null_df.any(axis=0)\n",
    "columns_with_null_values = test.columns[columns_with_null]\n",
    "\n",
    "print(columns_with_null_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['monday', 'tuesday', 'friday', 'saturday', 'sunday', 'thursday',\n",
       "       'wednesday'], dtype=object)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[\"x5\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"x0\":0.042317,\"x1\":-3.344721,\"x2\":4.6351242122,\"x3\":-0.5983959993,\"x4\":-0.6477715046,\"x5\":\"monday\",\"x6\":0.184902,\"x7\":46.690015,\"x8\":3.034132,\"x9\":0.364704,\"x10\":14.260733,\"x11\":-1.559332,\"x12\":\"$5,547.78\",\"x13\":0.520324,\"x14\":31.212255,\"x15\":4.891671,\"x16\":0.357763,\"x17\":14.766366,\"x18\":-17.467243,\"x19\":0.224628,\"x20\":0.096752,\"x21\":1.305564,\"x22\":0.353632,\"x23\":3.909028,\"x24\":-91.273052,\"x25\":1.396952,\"x26\":4.401593,\"x27\":0.443086,\"x28\":14.048787,\"x29\":-0.932243,\"x30\":5.255472,\"x31\":\"germany\",\"x32\":0.54199153,\"x33\":2.98948039,\"x34\":-1.78334189,\"x35\":0.80127315,\"x36\":-2.60231221,\"x37\":3.39682926,\"x38\":-1.22322646,\"x39\":-2.20977636,\"x40\":-68.69,\"x41\":522.25,\"x42\":-428.69,\"x43\":381.37,\"x44\":0.0197503,\"x45\":0.75116479,\"x46\":0.8630479008,\"x47\":-1.0383166613,\"x48\":-0.2726187635,\"x49\":-0.3430207259,\"x50\":0.3109008666,\"x51\":-0.797841974,\"x52\":-2.0390175153,\"x53\":0.87182889,\"x54\":0.14373012,\"x55\":-1.15212514,\"x56\":-2.1703139704,\"x57\":-0.267842962,\"x58\":0.212110633,\"x59\":1.6926559407,\"x60\":-0.9522767913,\"x61\":-0.8625864974,\"x62\":0.0748487158,\"x63\":\"36.29%\",\"x64\":3.47125327,\"x65\":-3.16656509,\"x66\":0.65446814,\"x67\":14.60067029,\"x68\":-20.57521013,\"x69\":0.71083785,\"x70\":0.16983767,\"x71\":0.55082127,\"x72\":0.62814576,\"x73\":3.38608078,\"x74\":-112.45263714,\"x75\":1.48370808,\"x76\":1.77035368,\"x77\":0.75702363,\"x78\":14.75731742,\"x79\":-0.62550355,\"x80\":null,\"x81\":\"October\",\"x82\":\"Female\",\"x83\":-0.7116680715,\"x84\":-0.2653559892,\"x85\":0.5175495907,\"x86\":-1.0881027092,\"x87\":-1.8188638198,\"x88\":-1.3584469527,\"x89\":-0.654995195,\"x90\":-0.4933042262,\"x91\":0.373853,\"x92\":0.94143481,\"x93\":3.54679834,\"x94\":-99.8574882,\"x95\":0.403926,\"x96\":1.65378726,\"x97\":0.00771459,\"x98\":-32.02164582,\"x99\":-60.3127828}'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(\"statefarm/files/data/exercise_26_test.csv\")\n",
    "\n",
    "# Selecting the first two rows and dropping the target column 'y'\n",
    "sample_data = test.head(10)\n",
    "\n",
    "# Convert these rows to JSON\n",
    "sample_json_single = sample_data.iloc[0].to_json()\n",
    "sample_json_batch = sample_data.to_json(orient=\"records\")\n",
    "sample_df = pd.DataFrame([json.loads(sample_json_single)])\n",
    "sample_json_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prediction' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/_n/rcr168w15p5f9f5llsd0qrg00000gn/T/ipykernel_24427/3573157969.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Preparing the final response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0msample_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'records'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'business_outcome'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'phat'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprobability\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Sorting the keys alphabetically\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'prediction' is not defined"
     ]
    }
   ],
   "source": [
    "response = pd.DataFrame(\n",
    "    model.final_result.predict(preprocessor.transform(sample_df)[model.variables])\n",
    ").rename(columns={0: \"phat\"})\n",
    "response[\"variable_name\"] = \"business_outcome\"\n",
    "\n",
    "# Preparing the final response\n",
    "response = {\n",
    "    **sample_df.to_dict(orient=\"records\")[0],\n",
    "    \"business_outcome\": prediction[0],\n",
    "    \"phat\": probability[0],\n",
    "}\n",
    "\n",
    "# Sorting the keys alphabetically\n",
    "response = dict(sorted(response.items()))\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"statefarm/files/models/logistic_regression_model.pkl\"\n",
    "PREPROCESSOR_PATH = \"statefarm/files/models/preprocessor.pkl\"\n",
    "\n",
    "model = joblib.load(MODEL_PATH)\n",
    "preprocessor = joblib.load(PREPROCESSOR_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.final_result.predict(preprocessor.transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val = pd.read_csv(\"statefarm/files/data/exercise_26_train.csv\")\n",
    "\n",
    "train_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debrief\n",
    "In the final discussion with the business partner, the partner was thrilled with the rank-order ability of the model.  Based on a combination of capacity and accuracy, the partner would like to classify any observation that would fall in the top 5 bins as an event; for simplicity we will say the cutoff is at the 75th percentile.  For the API, please return the predicted outcome (variable name is business_outcome), predicted probability (variable name is phat), and all model inputs; the variables should be returned in alphabetical order in the API return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overview of data types\n",
    "print(\"object dtype:\", raw_train.columns[raw_train.dtypes == \"object\"].tolist())\n",
    "print(\"int64 dtype:\", raw_train.columns[raw_train.dtypes == \"int\"].tolist())\n",
    "print(\"The rest of the columns have float64 dtypes.\")\n",
    "\n",
    "# Investigate Object Columns\n",
    "def investigate_object(df):\n",
    "    \"\"\"\n",
    "    This function prints the unique categories of all the object dtype columns.\n",
    "    It prints '...' if there are more than 13 unique categories.\n",
    "    \"\"\"\n",
    "    col_obj = df.columns[df.dtypes == \"object\"]\n",
    "\n",
    "    for i in range(len(col_obj)):\n",
    "        if len(df[col_obj[i]].unique()) > 13:\n",
    "            print(\n",
    "                col_obj[i] + \":\",\n",
    "                \"Unique Values:\",\n",
    "                np.append(df[col_obj[i]].unique()[:13], \"...\"),\n",
    "            )\n",
    "        else:\n",
    "            print(col_obj[i] + \":\", \"Unique Values:\", df[col_obj[i]].unique())\n",
    "\n",
    "    del col_obj\n",
    "\n",
    "\n",
    "investigate_object(raw_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
